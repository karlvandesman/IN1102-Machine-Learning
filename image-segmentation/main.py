import argparse
import numpy as np

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.metrics import accuracy_score
from scipy.stats import wilcoxon

from models.bayesian_gaussian import GaussianBayes
from models.bayesian_knn import BayesianKNN
from datasetHelper import Dataset

def main(args):
    
    if args.command == 'bayesian_gaussian':
        bayesian_gaussian();

    elif args.command == 'bayesian_knn':
        bayesian_knn();

    else:
        bayesian_gaussian();
        bayesian_knn();

def bayesian_gaussian():
    
    dataset = Dataset();

    # Split Features and Classes
#    X = dataset.train_dataset.values;
#    y = dataset.train_dataset.index;
    
    X = dataset.test_dataset.values;
    y = dataset.test_dataset.index;    
    
    # Normalizing the data
    standard_scaler = StandardScaler()
    X = standard_scaler.fit_transform(X)
        
    classes, num_classes = np.unique(np.sort(y), return_counts=True);
    
    # Transform labels - categoric <->  numeric
    encoding = LabelEncoder();
    encoding.classes_ = classes;

    y = encoding.transform(y);
    classes = encoding.transform(classes);
    
    # Parameters
    L = 2	# quantity of views
    seed = 10
    max_k_neighbors = 20
    k = len(classes)
    
    # Cross-validation: "30 times ten-fold"
    n_folds = 10
    n_repeats = 30

    rkf = RepeatedStratifiedKFold(n_splits=n_folds, n_repeats=n_repeats, 
                                  random_state=seed);
    
    accuracy_gaussian = []
    accuracy_KNN = []

    for train_index, test_index in rkf.split(X, y):
        X_train, X_test = X[train_index], X[test_index];
        y_train, y_test = y[train_index], y[test_index];
       
        X_train_shape, X_train_RGB = dataset.split_views(X_train);
        X_test_shape, X_test_RGB = dataset.split_views(X_test);
        
        classes, num_classes = np.unique(np.sort(y_train), return_counts=True)
        PClasses = num_classes/len(y_train)
        
        bayesian_gaussian_shape = GaussianBayes();
        bayesian_gaussian_RGB = GaussianBayes();
        
        bayesian_KNN_shape = BayesianKNN()
        bayesian_KNN_RGB = BayesianKNN()
        
        bayesian_gaussian_shape.fit(X_train_shape, y_train, classes)
        bayesian_gaussian_RGB.fit(X_train_RGB, y_train, classes)
        
        bayesian_KNN_shape.fit(X_train_shape, y_train)
        bayesian_KNN_RGB.fit(X_train_RGB, y_train)
                
        prob_gaussian_shape = bayesian_gaussian_shape.predict_prob(X_test_shape)
        prob_gaussian_RGB = bayesian_gaussian_RGB.predict_prob(X_test_RGB)
                
        prob_KNN_shape = bayesian_KNN_shape.predict_prob(X_test_shape)
        prob_KNN_RGB = bayesian_KNN_RGB.predict_prob(X_test_RGB)
        
        # Using the shape and RGB views to combine the classifiers
        probClf1 = [(1 - L) * PClasses[j] + prob_gaussian_shape[:, j]
                    + prob_gaussian_RGB[:, j] for j in range(k) ]
        
        probClf2 = [ (1 - L) * PClasses[j] + prob_KNN_shape[:, j] 
                    + prob_KNN_RGB[:, j] for j in range(k) ]
        
        # Fixing the shape
        probClf1 = np.vstack(probClf1).T 
        probClf2 = np.vstack(probClf2).T
        
        # Get the class with highest probbability
        y_pred_gaussian = np.argmax(probClf1, axis=1)
        y_pred_KNN = np.argmax(probClf2, axis=1)
        
        accuracy_gaussian.append(accuracy_score(y_test, y_pred_gaussian))
        accuracy_KNN.append(accuracy_score(y_test, y_pred_KNN))
    
    # Now we get the mean for the repeated K-folds
    accuracy_KFold_gaussian = \
                np.asarray([ np.mean(accuracy_gaussian[i:i+n_folds])
                            for i in range(0, n_folds*n_repeats, n_folds) ])
    
    accuracy_KFold_KNN = \
                np.asarray([ np.mean(accuracy_KNN[i:i+n_folds])
                            for i in range(0, n_folds*n_repeats, n_folds) ])
        
    print("KFold accuracy gaussian bayesian:\n", accuracy_KFold_gaussian)
    print()
    print("KFold accuracy kNN bayesian:\n", accuracy_KFold_KNN)
    print()
    
    # Comparing the values generated by the RepeatedStratifiedKFold
    # The Wilcoxon test computes di = xi - yi
    # H0: accuracyGaussian > accuracyKNN
    # H1: accuracyGaussian < accuracyKNN
    statistic, pvalue = wilcoxon(accuracy_KFold_gaussian, accuracy_KFold_KNN)
    
    print('Statistics=%.9f, p-value=%.9f' % (statistic, pvalue))
    print()
    
    alpha = 0.05
    
    if(pvalue>alpha):
    	print("We have strong evidence ( alpha =", alpha,
    		") that Gaussian classifier performed better than the kNN")
    else:
    	print("We have strong evidence ( alpha =", 
    		alpha, ") that kNN classifier performed better than the Gaussian")

def bayesian_knn():
    pass;

if __name__ == '__main__':
    parser = argparse.ArgumentParser();
    parser.add_argument('-c', '--command', type=str, 
                        help=('enter a command [gaussian, knn] to run the'
                        'specific model. Default is [all]'), 
                        action='store', default='all');
    args = parser.parse_args();

    main(args);
    