import argparse
import numpy as np

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.metrics import accuracy_score
from scipy.stats import wilcoxon

from models.bayesian_gaussian import GaussianBayes
from models.bayesian_knn import BayesianKNN
from datasetHelper import Dataset

def main(args):
    
    if args.command == 'bayesian_gaussian':
        bayesian_gaussian();

    elif args.command == 'bayesian_knn':
        bayesian_knn();

    else:
        bayesian_gaussian();
        bayesian_knn();

def bayesian_gaussian():
    
    dataset = Dataset();

    # Split Features and Classes
    X = dataset.train_dataset.values;
    y = dataset.train_dataset.index;
    
    # Normalizing the data
    standard_scaler = StandardScaler()
    X = standard_scaler.fit_transform(X)
        
    classes, num_classes = np.unique(np.sort(y), return_counts=True);
    
    # Transform labels - categoric <->  numeric
    encoding = LabelEncoder();
    encoding.classes_ = classes;

    y = encoding.transform(y);
    classes = encoding.transform(classes);
    
    # Parameters
    L = 2	# quantity of views
    seed = 10
    max_k_neighbors = 20
    k = len(classes)
    PClasses = num_classes/len(dataset)
    
    # Cross-validation: "30 times ten-fold"
    n_folds = 10
    n_repeats = 30

    rkf = RepeatedStratifiedKFold(n_splits=n_folds, n_repeats=n_repeats, 
                                  random_state=seed);
    
    accuracyGaussian = []
    accuracyKNN = []

    for train_index, test_index in rkf.split(X, y):
        X_train, X_val = X[train_index], X[test_index];
        y_train, y_val = y[train_index], y[test_index];
       
        X_train_shape, X_train_RGB = dataset.split_views(X_train);
        X_val_shape, X_val_RGB = dataset.split_views(X_val);
        
        shapeGb = GaussianBayes();
        RGBGb = GaussianBayes();
        
        bayesianKNN_shape = BayesianKNN()
        bayesianKNN_RGB = BayesianKNN()
        
        print('training...')        
        shapeGb.fit(X_train_shape, y_train, classes);
        RGBGb.fit(X_train_RGB, y_train, classes);
        
        bayesianKNN_shape.fit(X_train_shape, y_train)
        bayesianKNN_RGB.fit(X_train_shape, y_train)
        
        print('testing...')
        shapeGb.predict(X_val_shape);
        RGBGb.predict(X_val_RGB);
        
        prob_KNN_shape = bayesianKNN_shape.predict_prob(X_train_shape)
        prob_KNN_RGB = bayesianKNN_RGB.predict_prob(X_val_RGB)
        
        probClf1 = [(1 - L) * PClasses[j] + shapeGb.predicted_set[:, j] + 
                    RGBGb.predicted_set[:, j] for j in range(k) ]
        
        probClf2 = [ (1 - L) * PClasses[j] + prob_KNN_shape[:, j] 
                    + prob_KNN_RGB[:, j] for j in range(k) ]

        probClf2 = np.vstack(probClf2).T 
        
        # Index of the class with the higher prob  
        y_pred_gaussian = np.argmax(probClf1, axis=1);
        y_pred_KNN = np.argmax(probClf2, axis=1)
        
        accuracyGaussian.append(accuracy_score(y_val, y_pred_gaussian));
        accuracyKNN.append(accuracy_score(y_val, y_pred_KNN))
    
    # Now we get the mean for the repeated K-folds
    gaussian_KFold = np.asarray([ np.mean(accuracyGaussian[i:i+10]) \
                                 for i in range(0, n_folds*n_repeats, 10) ]);
    
    kNN_KFold = np.asarray([ np.mean(accuracyGaussian[i:i+10]) \
                            for i in range(0, n_folds*n_repeats, 10) ]);
    print(gaussian_KFold);
    print('Gaussian Model Finished...')

    # Comparing the values generated by the RepeatedStratifiedKFold
    # The Wilcoxon test computes di = xi - yi
    # H0: accuracyGaussian > accuracyKNN
    # H1: accuracyGaussian < accuracyKNN
    statistic, pvalue = wilcoxon(gaussian_KFold, kNN_KFold)
    
    print('Statistics=%.3f, p-value=%.3f' % (statistic, pvalue))
    print()
    
    alpha = 0.05
    
    if(pvalue>alpha):
    	print("We have strong evidence ( alpha =", alpha,
    		") that Gaussian classifier performed better than the kNN")
    else:
    	print("We have strong evidence ( alpha =", 
    		alpha, ") that kNN classifier performed better than the Gaussian")

def bayesian_knn():
    pass;

if __name__ == '__main__':
    parser = argparse.ArgumentParser();
    parser.add_argument('-c', '--command', type=str, 
                        help=('enter a command [gaussian, knn] to run the'
                        'specific model. Default is [all]'), 
                        action='store', default='all');
    args = parser.parse_args();

    main(args);
    